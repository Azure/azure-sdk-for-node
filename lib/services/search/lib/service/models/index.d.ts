/*
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License. See License.txt in the project root for
 * license information.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is
 * regenerated.
 */

import { BaseResource } from 'ms-rest-azure';
import { CloudError } from 'ms-rest-azure';
import * as moment from 'moment';

export { BaseResource } from 'ms-rest-azure';
export { CloudError } from 'ms-rest-azure';


/**
 * @class
 * Initializes a new instance of the AnalyzeRequest class.
 * @constructor
 * Specifies some text and analysis components used to break that text into
 * tokens.
 *
 * @member {string} text The text to break into tokens.
 * @member {object} [analyzer] The name of the analyzer to use to break the
 * given text. If this parameter is not specified, you must specify a tokenizer
 * instead. The tokenizer and analyzer parameters are mutually exclusive.
 * @member {string} [analyzer.name]
 * @member {object} [tokenizer] The name of the tokenizer to use to break the
 * given text. If this parameter is not specified, you must specify an analyzer
 * instead. The tokenizer and analyzer parameters are mutually exclusive.
 * @member {string} [tokenizer.name]
 * @member {array} [tokenFilters] An optional list of token filters to use when
 * breaking the given text. This parameter can only be set when using the
 * tokenizer parameter.
 * @member {array} [charFilters] An optional list of character filters to use
 * when breaking the given text. This parameter can only be set when using the
 * tokenizer parameter.
 */
export interface AnalyzeRequest {
  text: string;
  analyzer?: AnalyzerName;
  tokenizer?: TokenizerName;
  tokenFilters?: TokenFilterName[];
  charFilters?: CharFilterName[];
}

/**
 * @class
 * Initializes a new instance of the TokenInfo class.
 * @constructor
 * Information about a token returned by an analyzer.
 *
 * @member {string} [token] The token returned by the analyzer.
 * @member {number} [startOffset] The index of the first character of the token
 * in the input text.
 * @member {number} [endOffset] The index of the last character of the token in
 * the input text.
 * @member {number} [position] The position of the token in the input text
 * relative to other tokens. The first token in the input text has position 0,
 * the next has position 1, and so on. Depending on the analyzer used, some
 * tokens might have the same position, for example if they are synonyms of
 * each other.
 */
export interface TokenInfo {
  readonly token?: string;
  readonly startOffset?: number;
  readonly endOffset?: number;
  readonly position?: number;
}

/**
 * @class
 * Initializes a new instance of the AnalyzeResult class.
 * @constructor
 * The result of testing an analyzer on text.
 *
 * @member {array} [tokens] The list of tokens returned by the analyzer
 * specified in the request.
 */
export interface AnalyzeResult {
  tokens?: TokenInfo[];
}

/**
 * @class
 * Initializes a new instance of the Analyzer class.
 * @constructor
 * Abstract base class for analyzers.
 *
 * @member {string} name The name of the analyzer. It must only contain
 * letters, digits, spaces, dashes or underscores, can only start and end with
 * alphanumeric characters, and is limited to 128 characters.
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface Analyzer {
  name: string;
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the CustomAnalyzer class.
 * @constructor
 * Allows you to take control over the process of converting text into
 * indexable/searchable tokens. It's a user-defined configuration consisting of
 * a single predefined tokenizer and one or more filters. The tokenizer is
 * responsible for breaking text into tokens, and the filters for modifying
 * tokens emitted by the tokenizer.
 *
 * @member {object} tokenizer The name of the tokenizer to use to divide
 * continuous text into a sequence of tokens, such as breaking a sentence into
 * words.
 * @member {string} [tokenizer.name]
 * @member {array} [tokenFilters] A list of token filters used to filter out or
 * modify the tokens generated by a tokenizer. For example, you can specify a
 * lowercase filter that converts all characters to lowercase. The filters are
 * run in the order in which they are listed.
 * @member {array} [charFilters] A list of character filters used to prepare
 * input text before it is processed by the tokenizer. For instance, they can
 * replace certain characters or symbols. The filters are run in the order in
 * which they are listed.
 */
export interface CustomAnalyzer extends Analyzer {
  tokenizer: TokenizerName;
  tokenFilters?: TokenFilterName[];
  charFilters?: CharFilterName[];
}

/**
 * @class
 * Initializes a new instance of the PatternAnalyzer class.
 * @constructor
 * Flexibly separates text into terms via a regular expression pattern. This
 * analyzer is implemented using Apache Lucene.
 *
 * @member {boolean} [lowerCaseTerms] A value indicating whether terms should
 * be lower-cased. Default is true. Default value: true .
 * @member {string} [pattern] A regular expression pattern to match token
 * separators. Default is an expression that matches one or more whitespace
 * characters. Default value: '\W+' .
 * @member {object} [flags] Regular expression flags.
 * @member {string} [flags.name]
 * @member {array} [stopwords] A list of stopwords.
 */
export interface PatternAnalyzer extends Analyzer {
  lowerCaseTerms?: boolean;
  pattern?: string;
  flags?: RegexFlags;
  stopwords?: string[];
}

/**
 * @class
 * Initializes a new instance of the StandardAnalyzer class.
 * @constructor
 * Standard Apache Lucene analyzer; Composed of the standard tokenizer,
 * lowercase filter and stop filter.
 *
 * @member {number} [maxTokenLength] The maximum token length. Default is 255.
 * Tokens longer than the maximum length are split. The maximum token length
 * that can be used is 300 characters. Default value: 255 .
 * @member {array} [stopwords] A list of stopwords.
 */
export interface StandardAnalyzer extends Analyzer {
  maxTokenLength?: number;
  stopwords?: string[];
}

/**
 * @class
 * Initializes a new instance of the StopAnalyzer class.
 * @constructor
 * Divides text at non-letters; Applies the lowercase and stopword token
 * filters. This analyzer is implemented using Apache Lucene.
 *
 * @member {array} [stopwords] A list of stopwords.
 */
export interface StopAnalyzer extends Analyzer {
  stopwords?: string[];
}

/**
 * @class
 * Initializes a new instance of the Tokenizer class.
 * @constructor
 * Abstract base class for tokenizers.
 *
 * @member {string} name The name of the tokenizer. It must only contain
 * letters, digits, spaces, dashes or underscores, can only start and end with
 * alphanumeric characters, and is limited to 128 characters.
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface Tokenizer {
  name: string;
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the ClassicTokenizer class.
 * @constructor
 * Grammar-based tokenizer that is suitable for processing most
 * European-language documents. This tokenizer is implemented using Apache
 * Lucene.
 *
 * @member {number} [maxTokenLength] The maximum token length. Default is 255.
 * Tokens longer than the maximum length are split. The maximum token length
 * that can be used is 300 characters. Default value: 255 .
 */
export interface ClassicTokenizer extends Tokenizer {
  maxTokenLength?: number;
}

/**
 * @class
 * Initializes a new instance of the KeywordTokenizerV2 class.
 * @constructor
 * Emits the entire input as a single token.
 *
 * @member {number} [maxTokenLength] The maximum token length. Default is 256.
 * Tokens longer than the maximum length are split. The maximum token length
 * that can be used is 300 characters. Default value: 256 .
 */
export interface KeywordTokenizerV2 extends Tokenizer {
  maxTokenLength?: number;
}

/**
 * @class
 * Initializes a new instance of the MicrosoftLanguageTokenizer class.
 * @constructor
 * Divides text using language-specific rules.
 *
 * @member {number} [maxTokenLength] The maximum token length. Tokens longer
 * than the maximum length are split. Maximum token length that can be used is
 * 300 characters. Tokens longer than 300 characters are first split into
 * tokens of length 300 and then each of those tokens is split based on the max
 * token length set. Default is 255. Default value: 255 .
 * @member {boolean} [isSearchTokenizer] A value indicating how the tokenizer
 * is used. Set to true if used as the search tokenizer, set to false if used
 * as the indexing tokenizer. Default is false. Default value: false .
 * @member {string} [language] The language to use. The default is English.
 * Possible values include: 'bangla', 'bulgarian', 'catalan',
 * 'chineseSimplified', 'chineseTraditional', 'croatian', 'czech', 'danish',
 * 'dutch', 'english', 'french', 'german', 'greek', 'gujarati', 'hindi',
 * 'icelandic', 'indonesian', 'italian', 'japanese', 'kannada', 'korean',
 * 'malay', 'malayalam', 'marathi', 'norwegianBokmaal', 'polish', 'portuguese',
 * 'portugueseBrazilian', 'punjabi', 'romanian', 'russian', 'serbianCyrillic',
 * 'serbianLatin', 'slovenian', 'spanish', 'swedish', 'tamil', 'telugu',
 * 'thai', 'ukrainian', 'urdu', 'vietnamese'
 */
export interface MicrosoftLanguageTokenizer extends Tokenizer {
  maxTokenLength?: number;
  isSearchTokenizer?: boolean;
  language?: string;
}

/**
 * @class
 * Initializes a new instance of the MicrosoftLanguageStemmingTokenizer class.
 * @constructor
 * Divides text using language-specific rules and reduces words to their base
 * forms.
 *
 * @member {number} [maxTokenLength] The maximum token length. Tokens longer
 * than the maximum length are split. Maximum token length that can be used is
 * 300 characters. Tokens longer than 300 characters are first split into
 * tokens of length 300 and then each of those tokens is split based on the max
 * token length set. Default is 255. Default value: 255 .
 * @member {boolean} [isSearchTokenizer] A value indicating how the tokenizer
 * is used. Set to true if used as the search tokenizer, set to false if used
 * as the indexing tokenizer. Default is false. Default value: false .
 * @member {string} [language] The language to use. The default is English.
 * Possible values include: 'arabic', 'bangla', 'bulgarian', 'catalan',
 * 'croatian', 'czech', 'danish', 'dutch', 'english', 'estonian', 'finnish',
 * 'french', 'german', 'greek', 'gujarati', 'hebrew', 'hindi', 'hungarian',
 * 'icelandic', 'indonesian', 'italian', 'kannada', 'latvian', 'lithuanian',
 * 'malay', 'malayalam', 'marathi', 'norwegianBokmaal', 'polish', 'portuguese',
 * 'portugueseBrazilian', 'punjabi', 'romanian', 'russian', 'serbianCyrillic',
 * 'serbianLatin', 'slovak', 'slovenian', 'spanish', 'swedish', 'tamil',
 * 'telugu', 'turkish', 'ukrainian', 'urdu'
 */
export interface MicrosoftLanguageStemmingTokenizer extends Tokenizer {
  maxTokenLength?: number;
  isSearchTokenizer?: boolean;
  language?: string;
}

/**
 * @class
 * Initializes a new instance of the PatternTokenizer class.
 * @constructor
 * Tokenizer that uses regex pattern matching to construct distinct tokens.
 * This tokenizer is implemented using Apache Lucene.
 *
 * @member {string} [pattern] A regular expression pattern to match token
 * separators. Default is an expression that matches one or more whitespace
 * characters. Default value: '\W+' .
 * @member {object} [flags] Regular expression flags.
 * @member {string} [flags.name]
 * @member {number} [group] The zero-based ordinal of the matching group in the
 * regular expression pattern to extract into tokens. Use -1 if you want to use
 * the entire pattern to split the input into tokens, irrespective of matching
 * groups. Default is -1. Default value: -1 .
 */
export interface PatternTokenizer extends Tokenizer {
  pattern?: string;
  flags?: RegexFlags;
  group?: number;
}

/**
 * @class
 * Initializes a new instance of the StandardTokenizerV2 class.
 * @constructor
 * Breaks text following the Unicode Text Segmentation rules. This tokenizer is
 * implemented using Apache Lucene.
 *
 * @member {number} [maxTokenLength] The maximum token length. Default is 255.
 * Tokens longer than the maximum length are split. The maximum token length
 * that can be used is 300 characters. Default value: 255 .
 */
export interface StandardTokenizerV2 extends Tokenizer {
  maxTokenLength?: number;
}

/**
 * @class
 * Initializes a new instance of the UaxUrlEmailTokenizer class.
 * @constructor
 * Tokenizes urls and emails as one token. This tokenizer is implemented using
 * Apache Lucene.
 *
 * @member {number} [maxTokenLength] The maximum token length. Default is 255.
 * Tokens longer than the maximum length are split. The maximum token length
 * that can be used is 300 characters. Default value: 255 .
 */
export interface UaxUrlEmailTokenizer extends Tokenizer {
  maxTokenLength?: number;
}

/**
 * @class
 * Initializes a new instance of the TokenFilter class.
 * @constructor
 * Abstract base class for token filters.
 *
 * @member {string} name The name of the token filter. It must only contain
 * letters, digits, spaces, dashes or underscores, can only start and end with
 * alphanumeric characters, and is limited to 128 characters.
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface TokenFilter {
  name: string;
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the AsciiFoldingTokenFilter class.
 * @constructor
 * Converts alphabetic, numeric, and symbolic Unicode characters which are not
 * in the first 127 ASCII characters (the "Basic Latin" Unicode block) into
 * their ASCII equivalents, if such equivalents exist. This token filter is
 * implemented using Apache Lucene.
 *
 * @member {boolean} [preserveOriginal] A value indicating whether the original
 * token will be kept. Default is false. Default value: false .
 */
export interface AsciiFoldingTokenFilter extends TokenFilter {
  preserveOriginal?: boolean;
}

/**
 * @class
 * Initializes a new instance of the CommonGramTokenFilter class.
 * @constructor
 * Construct bigrams for frequently occurring terms while indexing. Single
 * terms are still indexed too, with bigrams overlaid. This token filter is
 * implemented using Apache Lucene.
 *
 * @member {array} commonWords The set of common words.
 * @member {boolean} [ignoreCase] A value indicating whether common words
 * matching will be case insensitive. Default is false. Default value: false .
 * @member {boolean} [useQueryMode] A value that indicates whether the token
 * filter is in query mode. When in query mode, the token filter generates
 * bigrams and then removes common words and single terms followed by a common
 * word. Default is false. Default value: false .
 */
export interface CommonGramTokenFilter extends TokenFilter {
  commonWords: string[];
  ignoreCase?: boolean;
  useQueryMode?: boolean;
}

/**
 * @class
 * Initializes a new instance of the DictionaryDecompounderTokenFilter class.
 * @constructor
 * Decomposes compound words found in many Germanic languages. This token
 * filter is implemented using Apache Lucene.
 *
 * @member {array} wordList The list of words to match against.
 * @member {number} [minWordSize] The minimum word size. Only words longer than
 * this get processed. Default is 5. Maximum is 300. Default value: 5 .
 * @member {number} [minSubwordSize] The minimum subword size. Only subwords
 * longer than this are outputted. Default is 2. Maximum is 300. Default value:
 * 2 .
 * @member {number} [maxSubwordSize] The maximum subword size. Only subwords
 * shorter than this are outputted. Default is 15. Maximum is 300. Default
 * value: 15 .
 * @member {boolean} [onlyLongestMatch] A value indicating whether to add only
 * the longest matching subword to the output. Default is false. Default value:
 * false .
 */
export interface DictionaryDecompounderTokenFilter extends TokenFilter {
  wordList: string[];
  minWordSize?: number;
  minSubwordSize?: number;
  maxSubwordSize?: number;
  onlyLongestMatch?: boolean;
}

/**
 * @class
 * Initializes a new instance of the EdgeNGramTokenFilterV2 class.
 * @constructor
 * Generates n-grams of the given size(s) starting from the front or the back
 * of an input token. This token filter is implemented using Apache Lucene.
 *
 * @member {number} [minGram] The minimum n-gram length. Default is 1. Maximum
 * is 300. Must be less than the value of maxGram. Default value: 1 .
 * @member {number} [maxGram] The maximum n-gram length. Default is 2. Maximum
 * is 300. Default value: 2 .
 * @member {string} [side] Specifies which side of the input the n-gram should
 * be generated from. Default is "front". Possible values include: 'front',
 * 'back'
 */
export interface EdgeNGramTokenFilterV2 extends TokenFilter {
  minGram?: number;
  maxGram?: number;
  side?: string;
}

/**
 * @class
 * Initializes a new instance of the ElisionTokenFilter class.
 * @constructor
 * Removes elisions. For example, "l'avion" (the plane) will be converted to
 * "avion" (plane). This token filter is implemented using Apache Lucene.
 *
 * @member {array} [articles] The set of articles to remove.
 */
export interface ElisionTokenFilter extends TokenFilter {
  articles?: string[];
}

/**
 * @class
 * Initializes a new instance of the KeepTokenFilter class.
 * @constructor
 * A token filter that only keeps tokens with text contained in a specified
 * list of words. This token filter is implemented using Apache Lucene.
 *
 * @member {array} keepWords The list of words to keep.
 * @member {boolean} [lowerCaseKeepWords] A value indicating whether to lower
 * case all words first. Default is false. Default value: false .
 */
export interface KeepTokenFilter extends TokenFilter {
  keepWords: string[];
  lowerCaseKeepWords?: boolean;
}

/**
 * @class
 * Initializes a new instance of the KeywordMarkerTokenFilter class.
 * @constructor
 * Marks terms as keywords. This token filter is implemented using Apache
 * Lucene.
 *
 * @member {array} keywords A list of words to mark as keywords.
 * @member {boolean} [ignoreCase] A value indicating whether to ignore case. If
 * true, all words are converted to lower case first. Default is false. Default
 * value: false .
 */
export interface KeywordMarkerTokenFilter extends TokenFilter {
  keywords: string[];
  ignoreCase?: boolean;
}

/**
 * @class
 * Initializes a new instance of the LengthTokenFilter class.
 * @constructor
 * Removes words that are too long or too short. This token filter is
 * implemented using Apache Lucene.
 *
 * @member {number} [min] The minimum length in characters. Default is 0.
 * Maximum is 300. Must be less than the value of max. Default value: 0 .
 * @member {number} [max] The maximum length in characters. Default and maximum
 * is 300. Default value: 300 .
 */
export interface LengthTokenFilter extends TokenFilter {
  min?: number;
  max?: number;
}

/**
 * @class
 * Initializes a new instance of the LimitTokenFilter class.
 * @constructor
 * Limits the number of tokens while indexing. This token filter is implemented
 * using Apache Lucene.
 *
 * @member {number} [maxTokenCount] The maximum number of tokens to produce.
 * Default is 1. Default value: 1 .
 * @member {boolean} [consumeAllTokens] A value indicating whether all tokens
 * from the input must be consumed even if maxTokenCount is reached. Default is
 * false. Default value: false .
 */
export interface LimitTokenFilter extends TokenFilter {
  maxTokenCount?: number;
  consumeAllTokens?: boolean;
}

/**
 * @class
 * Initializes a new instance of the NGramTokenFilterV2 class.
 * @constructor
 * Generates n-grams of the given size(s). This token filter is implemented
 * using Apache Lucene.
 *
 * @member {number} [minGram] The minimum n-gram length. Default is 1. Maximum
 * is 300. Must be less than the value of maxGram. Default value: 1 .
 * @member {number} [maxGram] The maximum n-gram length. Default is 2. Maximum
 * is 300. Default value: 2 .
 */
export interface NGramTokenFilterV2 extends TokenFilter {
  minGram?: number;
  maxGram?: number;
}

/**
 * @class
 * Initializes a new instance of the PatternCaptureTokenFilter class.
 * @constructor
 * Uses Java regexes to emit multiple tokens - one for each capture group in
 * one or more patterns. This token filter is implemented using Apache Lucene.
 *
 * @member {array} patterns A list of patterns to match against each token.
 * @member {boolean} [preserveOriginal] A value indicating whether to return
 * the original token even if one of the patterns matches. Default is true.
 * Default value: true .
 */
export interface PatternCaptureTokenFilter extends TokenFilter {
  patterns: string[];
  preserveOriginal?: boolean;
}

/**
 * @class
 * Initializes a new instance of the PatternReplaceTokenFilter class.
 * @constructor
 * A character filter that replaces characters in the input string. It uses a
 * regular expression to identify character sequences to preserve and a
 * replacement pattern to identify characters to replace. For example, given
 * the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This token filter is implemented
 * using Apache Lucene.
 *
 * @member {string} pattern A regular expression pattern.
 * @member {string} replacement The replacement text.
 */
export interface PatternReplaceTokenFilter extends TokenFilter {
  pattern: string;
  replacement: string;
}

/**
 * @class
 * Initializes a new instance of the PhoneticTokenFilter class.
 * @constructor
 * Create tokens for phonetic matches. This token filter is implemented using
 * Apache Lucene.
 *
 * @member {string} [encoder] The phonetic encoder to use. Default is
 * "metaphone". Possible values include: 'metaphone', 'doubleMetaphone',
 * 'soundex', 'refinedSoundex', 'caverphone1', 'caverphone2', 'cologne',
 * 'nysiis', 'koelnerPhonetik', 'haasePhonetik', 'beiderMorse'
 * @member {boolean} [replaceOriginalTokens] A value indicating whether encoded
 * tokens should replace original tokens. If false, encoded tokens are added as
 * synonyms. Default is true. Default value: true .
 */
export interface PhoneticTokenFilter extends TokenFilter {
  encoder?: string;
  replaceOriginalTokens?: boolean;
}

/**
 * @class
 * Initializes a new instance of the ShingleTokenFilter class.
 * @constructor
 * Creates combinations of tokens as a single token. This token filter is
 * implemented using Apache Lucene.
 *
 * @member {number} [maxShingleSize] The maximum shingle size. Default and
 * minimum value is 2. Default value: 2 .
 * @member {number} [minShingleSize] The minimum shingle size. Default and
 * minimum value is 2. Must be less than the value of maxShingleSize. Default
 * value: 2 .
 * @member {boolean} [outputUnigrams] A value indicating whether the output
 * stream will contain the input tokens (unigrams) as well as shingles. Default
 * is true. Default value: true .
 * @member {boolean} [outputUnigramsIfNoShingles] A value indicating whether to
 * output unigrams for those times when no shingles are available. This
 * property takes precedence when outputUnigrams is set to false. Default is
 * false. Default value: false .
 * @member {string} [tokenSeparator] The string to use when joining adjacent
 * tokens to form a shingle. Default is a single space (" "). Default value: ''
 * .
 * @member {string} [filterToken] The string to insert for each position at
 * which there is no token. Default is an underscore ("_"). Default value: '_'
 * .
 */
export interface ShingleTokenFilter extends TokenFilter {
  maxShingleSize?: number;
  minShingleSize?: number;
  outputUnigrams?: boolean;
  outputUnigramsIfNoShingles?: boolean;
  tokenSeparator?: string;
  filterToken?: string;
}

/**
 * @class
 * Initializes a new instance of the SnowballTokenFilter class.
 * @constructor
 * A filter that stems words using a Snowball-generated stemmer. This token
 * filter is implemented using Apache Lucene.
 *
 * @member {string} language The language to use. Possible values include:
 * 'armenian', 'basque', 'catalan', 'danish', 'dutch', 'english', 'finnish',
 * 'french', 'german', 'german2', 'hungarian', 'italian', 'kp', 'lovins',
 * 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish',
 * 'swedish', 'turkish'
 */
export interface SnowballTokenFilter extends TokenFilter {
  language: string;
}

/**
 * @class
 * Initializes a new instance of the StemmerTokenFilter class.
 * @constructor
 * Language specific stemming filter. This token filter is implemented using
 * Apache Lucene.
 *
 * @member {string} language The language to use. Possible values include:
 * 'arabic', 'armenian', 'basque', 'brazilian', 'bulgarian', 'catalan',
 * 'czech', 'danish', 'dutch', 'dutchKp', 'english', 'lightEnglish',
 * 'minimalEnglish', 'possessiveEnglish', 'porter2', 'lovins', 'finnish',
 * 'lightFinnish', 'french', 'lightFrench', 'minimalFrench', 'galician',
 * 'minimalGalician', 'german', 'german2', 'lightGerman', 'minimalGerman',
 * 'greek', 'hindi', 'hungarian', 'lightHungarian', 'indonesian', 'irish',
 * 'italian', 'lightItalian', 'sorani', 'latvian', 'norwegian',
 * 'lightNorwegian', 'minimalNorwegian', 'lightNynorsk', 'minimalNynorsk',
 * 'portuguese', 'lightPortuguese', 'minimalPortuguese', 'portugueseRslp',
 * 'romanian', 'russian', 'lightRussian', 'spanish', 'lightSpanish', 'swedish',
 * 'lightSwedish', 'turkish'
 */
export interface StemmerTokenFilter extends TokenFilter {
  language: string;
}

/**
 * @class
 * Initializes a new instance of the StemmerOverrideTokenFilter class.
 * @constructor
 * Provides the ability to override other stemming filters with custom
 * dictionary-based stemming. Any dictionary-stemmed terms will be marked as
 * keywords so that they will not be stemmed with stemmers down the chain. Must
 * be placed before any stemming filters. This token filter is implemented
 * using Apache Lucene.
 *
 * @member {array} rules A list of stemming rules in the following format:
 * "word => stem", for example: "ran => run"
 */
export interface StemmerOverrideTokenFilter extends TokenFilter {
  rules: string[];
}

/**
 * @class
 * Initializes a new instance of the StopwordsTokenFilter class.
 * @constructor
 * Removes stop words from a token stream. This token filter is implemented
 * using Apache Lucene.
 *
 * @member {array} [stopwords] The list of stopwords. This property and the
 * stopwords list property cannot both be set.
 * @member {string} [stopwordsList] A predefined list of stopwords to use. This
 * property and the stopwords property cannot both be set. Default is English.
 * Possible values include: 'arabic', 'armenian', 'basque', 'brazilian',
 * 'bulgarian', 'catalan', 'czech', 'danish', 'dutch', 'english', 'finnish',
 * 'french', 'galician', 'german', 'greek', 'hindi', 'hungarian', 'indonesian',
 * 'irish', 'italian', 'latvian', 'norwegian', 'persian', 'portuguese',
 * 'romanian', 'russian', 'sorani', 'spanish', 'swedish', 'thai', 'turkish'
 * @member {boolean} [ignoreCase] A value indicating whether to ignore case. If
 * true, all words are converted to lower case first. Default is false. Default
 * value: false .
 * @member {boolean} [removeTrailingStopWords] A value indicating whether to
 * ignore the last search term if it's a stop word. Default is true. Default
 * value: true .
 */
export interface StopwordsTokenFilter extends TokenFilter {
  stopwords?: string[];
  stopwordsList?: string;
  ignoreCase?: boolean;
  removeTrailingStopWords?: boolean;
}

/**
 * @class
 * Initializes a new instance of the SynonymTokenFilter class.
 * @constructor
 * Matches single or multi-word synonyms in a token stream. This token filter
 * is implemented using Apache Lucene.
 *
 * @member {array} synonyms A list of synonyms in following one of two formats:
 * 1. incredible, unbelievable, fabulous => amazing - all terms on the left
 * side of => symbol will be replaced with all terms on its right side; 2.
 * incredible, unbelievable, fabulous, amazing - comma separated list of
 * equivalent words. Set the expand option to change how this list is
 * interpreted.
 * @member {boolean} [ignoreCase] A value indicating whether to case-fold input
 * for matching. Default is false. Default value: false .
 * @member {boolean} [expand] A value indicating whether all words in the list
 * of synonyms (if => notation is not used) will map to one another. If true,
 * all words in the list of synonyms (if => notation is not used) will map to
 * one another. The following list: incredible, unbelievable, fabulous, amazing
 * is equivalent to: incredible, unbelievable, fabulous, amazing => incredible,
 * unbelievable, fabulous, amazing. If false, the following list: incredible,
 * unbelievable, fabulous, amazing will be equivalent to: incredible,
 * unbelievable, fabulous, amazing => incredible. Default is true. Default
 * value: true .
 */
export interface SynonymTokenFilter extends TokenFilter {
  synonyms: string[];
  ignoreCase?: boolean;
  expand?: boolean;
}

/**
 * @class
 * Initializes a new instance of the TruncateTokenFilter class.
 * @constructor
 * Truncates the terms to a specific length. This token filter is implemented
 * using Apache Lucene.
 *
 * @member {number} [length] The length at which terms will be truncated.
 * Default and maximum is 300. Default value: 300 .
 */
export interface TruncateTokenFilter extends TokenFilter {
  length?: number;
}

/**
 * @class
 * Initializes a new instance of the UniqueTokenFilter class.
 * @constructor
 * Filters out tokens with same text as the previous token. This token filter
 * is implemented using Apache Lucene.
 *
 * @member {boolean} [onlyOnSamePosition] A value indicating whether to remove
 * duplicates only at the same position. Default is false. Default value: false
 * .
 */
export interface UniqueTokenFilter extends TokenFilter {
  onlyOnSamePosition?: boolean;
}

/**
 * @class
 * Initializes a new instance of the WordDelimiterTokenFilter class.
 * @constructor
 * Splits words into subwords and performs optional transformations on subword
 * groups. This token filter is implemented using Apache Lucene.
 *
 * @member {boolean} [generateWordParts] A value indicating whether to generate
 * part words. If set, causes parts of words to be generated; for example
 * "AzureSearch" becomes "Azure" "Search". Default is true. Default value: true
 * .
 * @member {boolean} [generateNumberParts] A value indicating whether to
 * generate number subwords. Default is true. Default value: true .
 * @member {boolean} [catenateWords] A value indicating whether maximum runs of
 * word parts will be catenated. For example, if this is set to true,
 * "Azure-Search" becomes "AzureSearch". Default is false. Default value: false
 * .
 * @member {boolean} [catenateNumbers] A value indicating whether maximum runs
 * of number parts will be catenated. For example, if this is set to true,
 * "1-2" becomes "12". Default is false. Default value: false .
 * @member {boolean} [catenateAll] A value indicating whether all subword parts
 * will be catenated. For example, if this is set to true, "Azure-Search-1"
 * becomes "AzureSearch1". Default is false. Default value: false .
 * @member {boolean} [splitOnCaseChange] A value indicating whether to split
 * words on caseChange. For example, if this is set to true, "AzureSearch"
 * becomes "Azure" "Search". Default is true. Default value: true .
 * @member {boolean} [preserveOriginal] A value indicating whether original
 * words will be preserved and added to the subword list. Default is false.
 * Default value: false .
 * @member {boolean} [splitOnNumerics] A value indicating whether to split on
 * numbers. For example, if this is set to true, "Azure1Search" becomes "Azure"
 * "1" "Search". Default is true. Default value: true .
 * @member {boolean} [stemEnglishPossessive] A value indicating whether to
 * remove trailing "'s" for each subword. Default is true. Default value: true
 * .
 * @member {array} [protectedWords] A list of tokens to protect from being
 * delimited.
 */
export interface WordDelimiterTokenFilter extends TokenFilter {
  generateWordParts?: boolean;
  generateNumberParts?: boolean;
  catenateWords?: boolean;
  catenateNumbers?: boolean;
  catenateAll?: boolean;
  splitOnCaseChange?: boolean;
  preserveOriginal?: boolean;
  splitOnNumerics?: boolean;
  stemEnglishPossessive?: boolean;
  protectedWords?: string[];
}

/**
 * @class
 * Initializes a new instance of the CharFilter class.
 * @constructor
 * Abstract base class for character filters.
 *
 * @member {string} name The name of the char filter. It must only contain
 * letters, digits, spaces, dashes or underscores, can only start and end with
 * alphanumeric characters, and is limited to 128 characters.
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface CharFilter {
  name: string;
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the MappingCharFilter class.
 * @constructor
 * A character filter that applies mappings defined with the mappings option.
 * Matching is greedy (longest pattern matching at a given point wins).
 * Replacement is allowed to be the empty string. This character filter is
 * implemented using Apache Lucene.
 *
 * @member {array} mappings A list of mappings of the following format: "a=>b"
 * (all occurrences of the character "a" will be replaced with character "b").
 */
export interface MappingCharFilter extends CharFilter {
  mappings: string[];
}

/**
 * @class
 * Initializes a new instance of the PatternReplaceCharFilter class.
 * @constructor
 * A character filter that replaces characters in the input string. It uses a
 * regular expression to identify character sequences to preserve and a
 * replacement pattern to identify characters to replace. For example, given
 * the input text "aa bb aa bb", pattern "(aa)\s+(bb)", and replacement
 * "$1#$2", the result would be "aa#bb aa#bb". This character filter is
 * implemented using Apache Lucene.
 *
 * @member {string} pattern A regular expression pattern.
 * @member {string} replacement The replacement text.
 */
export interface PatternReplaceCharFilter extends CharFilter {
  pattern: string;
  replacement: string;
}

/**
 * @class
 * Initializes a new instance of the DataSourceCredentials class.
 * @constructor
 * Represents credentials that can be used to connect to a datasource.
 *
 * @member {string} connectionString The connection string for the datasource.
 */
export interface DataSourceCredentials {
  connectionString: string;
}

/**
 * @class
 * Initializes a new instance of the DataContainer class.
 * @constructor
 * Represents information about the entity (such as Azure SQL table or
 * DocumentDb collection) that will be indexed.
 *
 * @member {string} name The name of the table or view (for Azure SQL data
 * source) or collection (for DocumentDB data source) that will be indexed.
 * @member {string} [query] A query that is applied to this data container. The
 * syntax and meaning of this parameter is datasource-specific. Not supported
 * by Azure SQL datasources.
 */
export interface DataContainer {
  name: string;
  query?: string;
}

/**
 * @class
 * Initializes a new instance of the DataChangeDetectionPolicy class.
 * @constructor
 * Abstract base class for data change detection policies.
 *
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface DataChangeDetectionPolicy {
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the HighWaterMarkChangeDetectionPolicy class.
 * @constructor
 * Defines a data change detection policy that captures changes based on the
 * value of a high water mark column.
 *
 * @member {string} highWaterMarkColumnName The name of the high water mark
 * column.
 */
export interface HighWaterMarkChangeDetectionPolicy extends DataChangeDetectionPolicy {
  highWaterMarkColumnName: string;
}

/**
 * @class
 * Initializes a new instance of the SqlIntegratedChangeTrackingPolicy class.
 * @constructor
 * Defines a data change detection policy that captures changes using the
 * Integrated Change Tracking feature of Azure SQL Database.
 *
 */
export interface SqlIntegratedChangeTrackingPolicy extends DataChangeDetectionPolicy {
}

/**
 * @class
 * Initializes a new instance of the DataDeletionDetectionPolicy class.
 * @constructor
 * Abstract base class for data deletion detection policies.
 *
 * @member {string} odatatype Polymorphic Discriminator
 */
export interface DataDeletionDetectionPolicy {
  odatatype: string;
}

/**
 * @class
 * Initializes a new instance of the SoftDeleteColumnDeletionDetectionPolicy class.
 * @constructor
 * Defines a data deletion detection policy that implements a soft-deletion
 * strategy. It determines whether an item should be deleted based on the value
 * of a designated 'soft delete' column.
 *
 * @member {string} [softDeleteColumnName] The name of the column to use for
 * soft-deletion detection.
 * @member {string} [softDeleteMarkerValue] The marker value that indentifies
 * an item as deleted.
 */
export interface SoftDeleteColumnDeletionDetectionPolicy extends DataDeletionDetectionPolicy {
  softDeleteColumnName?: string;
  softDeleteMarkerValue?: string;
}

/**
 * @class
 * Initializes a new instance of the DataSource class.
 * @constructor
 * Represents a datasource definition in Azure Search, which can be used to
 * configure an indexer.
 *
 * @member {string} name The name of the datasource.
 * @member {string} [description] The description of the datasource.
 * @member {object} type The type of the datasource.
 * @member {string} [type.name]
 * @member {object} credentials Credentials for the datasource.
 * @member {string} [credentials.connectionString] The connection string for
 * the datasource.
 * @member {object} container The data container for the datasource.
 * @member {string} [container.name] The name of the table or view (for Azure
 * SQL data source) or collection (for DocumentDB data source) that will be
 * indexed.
 * @member {string} [container.query] A query that is applied to this data
 * container. The syntax and meaning of this parameter is datasource-specific.
 * Not supported by Azure SQL datasources.
 * @member {object} [dataChangeDetectionPolicy] The data change detection
 * policy for the datasource.
 * @member {string} [dataChangeDetectionPolicy.odatatype] Polymorphic
 * Discriminator
 * @member {object} [dataDeletionDetectionPolicy] The data deletion detection
 * policy for the datasource.
 * @member {string} [dataDeletionDetectionPolicy.odatatype] Polymorphic
 * Discriminator
 * @member {string} [eTag] The ETag of the DataSource.
 */
export interface DataSource {
  name: string;
  description?: string;
  type: DataSourceType;
  credentials: DataSourceCredentials;
  container: DataContainer;
  dataChangeDetectionPolicy?: DataChangeDetectionPolicy;
  dataDeletionDetectionPolicy?: DataDeletionDetectionPolicy;
  eTag?: string;
}

/**
 * @class
 * Initializes a new instance of the DataSourceListResult class.
 * @constructor
 * Response from a List Datasources request. If successful, it includes the
 * full definitions of all datasources.
 *
 * @member {array} [dataSources] The datasources in the Search service.
 */
export interface DataSourceListResult {
  readonly dataSources?: DataSource[];
}

/**
 * @class
 * Initializes a new instance of the IndexingSchedule class.
 * @constructor
 * Represents a schedule for indexer execution.
 *
 * @member {moment.duration} interval The interval of time between indexer
 * executions.
 * @member {date} [startTime] The time when an indexer should start running.
 */
export interface IndexingSchedule {
  interval: moment.Duration;
  startTime?: Date;
}

/**
 * @class
 * Initializes a new instance of the FieldMappingFunction class.
 * @constructor
 * Represents a function that transforms a value from a data source before
 * indexing.
 *
 * @member {string} name The name of the field mapping function.
 * @member {object} [parameters] A dictionary of parameter name/value pairs to
 * pass to the function. Each value must be of a primitive type.
 */
export interface FieldMappingFunction {
  name: string;
  parameters?: { [propertyName: string]: any };
}

/**
 * @class
 * Initializes a new instance of the FieldMapping class.
 * @constructor
 * Defines a mapping between a field in a data source and a target field in an
 * index.
 *
 * @member {string} sourceFieldName The name of the field in the data source.
 * @member {string} [targetFieldName] The name of the target field in the
 * index. Same as the source field name by default.
 * @member {object} [mappingFunction] A function to apply to each source field
 * value before indexing.
 * @member {string} [mappingFunction.name] The name of the field mapping
 * function.
 * @member {object} [mappingFunction.parameters] A dictionary of parameter
 * name/value pairs to pass to the function. Each value must be of a primitive
 * type.
 */
export interface FieldMapping {
  sourceFieldName: string;
  targetFieldName?: string;
  mappingFunction?: FieldMappingFunction;
}

/**
 * @class
 * Initializes a new instance of the Indexer class.
 * @constructor
 * Represents an Azure Search indexer.
 *
 * @member {string} name The name of the indexer.
 * @member {string} [description] The description of the indexer.
 * @member {string} dataSourceName The name of the datasource from which this
 * indexer reads data.
 * @member {string} targetIndexName The name of the index to which this indexer
 * writes data.
 * @member {object} [schedule] The schedule for this indexer.
 * @member {moment.duration} [schedule.interval] The interval of time between
 * indexer executions.
 * @member {date} [schedule.startTime] The time when an indexer should start
 * running.
 * @member {object} [parameters] Parameters for indexer execution.
 * @member {number} [parameters.batchSize] The number of items that are read
 * from the data source and indexed as a single batch in order to improve
 * performance. The default depends on the data source type.
 * @member {number} [parameters.maxFailedItems] The maximum number of items
 * that can fail indexing for indexer execution to still be considered
 * successful. -1 means no limit. Default is 0.
 * @member {number} [parameters.maxFailedItemsPerBatch] The maximum number of
 * items in a single batch that can fail indexing for the batch to still be
 * considered successful. -1 means no limit. Default is 0.
 * @member {boolean} [parameters.base64EncodeKeys] Whether indexer will
 * base64-encode all values that are inserted into key field of the target
 * index. This is needed if keys can contain characters that are invalid in
 * keys (such as dot '.'). Default is false.
 * @member {object} [parameters.configuration] A dictionary of indexer-specific
 * configuration properties. Each name is the name of a specific property. Each
 * value must be of a primitive type.
 * @member {array} [fieldMappings] Defines mappings between fields in the data
 * source and corresponding target fields in the index.
 * @member {boolean} [isDisabled] A value indicating whether the indexer is
 * disabled. Default is false. Default value: false .
 * @member {string} [eTag] The ETag of the Indexer.
 */
export interface Indexer {
  name: string;
  description?: string;
  dataSourceName: string;
  targetIndexName: string;
  schedule?: IndexingSchedule;
  parameters?: IndexingParameters;
  fieldMappings?: FieldMapping[];
  isDisabled?: boolean;
  eTag?: string;
}

/**
 * @class
 * Initializes a new instance of the IndexerListResult class.
 * @constructor
 * Response from a List Indexers request. If successful, it includes the full
 * definitions of all indexers.
 *
 * @member {array} [indexers] The indexers in the Search service.
 */
export interface IndexerListResult {
  readonly indexers?: Indexer[];
}

/**
 * @class
 * Initializes a new instance of the ItemError class.
 * @constructor
 * Represents an item- or document-level indexing error.
 *
 * @member {string} [key] The key of the item for which indexing failed.
 * @member {string} [errorMessage] The message describing the error that
 * occurred while attempting to index the item.
 */
export interface ItemError {
  readonly key?: string;
  readonly errorMessage?: string;
}

/**
 * @class
 * Initializes a new instance of the ScoringFunction class.
 * @constructor
 * Abstract base class for functions that can modify document scores during
 * ranking.
 *
 * @member {string} fieldName The name of the field used as input to the
 * scoring function.
 * @member {number} boost A multiplier for the raw score. Must be a positive
 * number not equal to 1.0.
 * @member {string} [interpolation] A value indicating how boosting will be
 * interpolated across document scores; defaults to "Linear". Possible values
 * include: 'linear', 'constant', 'quadratic', 'logarithmic'
 * @member {string} type Polymorphic Discriminator
 */
export interface ScoringFunction {
  fieldName: string;
  boost: number;
  interpolation?: string;
  type: string;
}

/**
 * @class
 * Initializes a new instance of the DistanceScoringParameters class.
 * @constructor
 * Provides parameter values to a distance scoring function.
 *
 * @member {string} referencePointParameter The name of the parameter passed in
 * search queries to specify the reference location.
 * @member {number} boostingDistance The distance in kilometers from the
 * reference location where the boosting range ends.
 */
export interface DistanceScoringParameters {
  referencePointParameter: string;
  boostingDistance: number;
}

/**
 * @class
 * Initializes a new instance of the DistanceScoringFunction class.
 * @constructor
 * Defines a function that boosts scores based on distance from a geographic
 * location.
 *
 * @member {object} parameters Parameter values for the distance scoring
 * function.
 * @member {string} [parameters.referencePointParameter] The name of the
 * parameter passed in search queries to specify the reference location.
 * @member {number} [parameters.boostingDistance] The distance in kilometers
 * from the reference location where the boosting range ends.
 */
export interface DistanceScoringFunction extends ScoringFunction {
  parameters: DistanceScoringParameters;
}

/**
 * @class
 * Initializes a new instance of the FreshnessScoringParameters class.
 * @constructor
 * Provides parameter values to a freshness scoring function.
 *
 * @member {moment.duration} boostingDuration The expiration period after which
 * boosting will stop for a particular document.
 */
export interface FreshnessScoringParameters {
  boostingDuration: moment.Duration;
}

/**
 * @class
 * Initializes a new instance of the FreshnessScoringFunction class.
 * @constructor
 * Defines a function that boosts scores based on the value of a date-time
 * field.
 *
 * @member {object} parameters Parameter values for the freshness scoring
 * function.
 * @member {moment.duration} [parameters.boostingDuration] The expiration
 * period after which boosting will stop for a particular document.
 */
export interface FreshnessScoringFunction extends ScoringFunction {
  parameters: FreshnessScoringParameters;
}

/**
 * @class
 * Initializes a new instance of the MagnitudeScoringParameters class.
 * @constructor
 * Provides parameter values to a magnitude scoring function.
 *
 * @member {number} boostingRangeStart The field value at which boosting
 * starts.
 * @member {number} boostingRangeEnd The field value at which boosting ends.
 * @member {boolean} [shouldBoostBeyondRangeByConstant] A value indicating
 * whether to apply a constant boost for field values beyond the range end
 * value; default is false.
 */
export interface MagnitudeScoringParameters {
  boostingRangeStart: number;
  boostingRangeEnd: number;
  shouldBoostBeyondRangeByConstant?: boolean;
}

/**
 * @class
 * Initializes a new instance of the MagnitudeScoringFunction class.
 * @constructor
 * Defines a function that boosts scores based on the magnitude of a numeric
 * field.
 *
 * @member {object} parameters Parameter values for the magnitude scoring
 * function.
 * @member {number} [parameters.boostingRangeStart] The field value at which
 * boosting starts.
 * @member {number} [parameters.boostingRangeEnd] The field value at which
 * boosting ends.
 * @member {boolean} [parameters.shouldBoostBeyondRangeByConstant] A value
 * indicating whether to apply a constant boost for field values beyond the
 * range end value; default is false.
 */
export interface MagnitudeScoringFunction extends ScoringFunction {
  parameters: MagnitudeScoringParameters;
}

/**
 * @class
 * Initializes a new instance of the TagScoringParameters class.
 * @constructor
 * Provides parameter values to a tag scoring function.
 *
 * @member {string} tagsParameter The name of the parameter passed in search
 * queries to specify the list of tags to compare against the target field.
 */
export interface TagScoringParameters {
  tagsParameter: string;
}

/**
 * @class
 * Initializes a new instance of the TagScoringFunction class.
 * @constructor
 * Defines a function that boosts scores of documents with string values
 * matching a given list of tags.
 *
 * @member {object} parameters Parameter values for the tag scoring function.
 * @member {string} [parameters.tagsParameter] The name of the parameter passed
 * in search queries to specify the list of tags to compare against the target
 * field.
 */
export interface TagScoringFunction extends ScoringFunction {
  parameters: TagScoringParameters;
}

/**
 * @class
 * Initializes a new instance of the ScoringProfile class.
 * @constructor
 * Defines parameters for an Azure Search index that influence scoring in
 * search queries.
 *
 * @member {string} name The name of the scoring profile.
 * @member {object} [textWeights] Parameters that boost scoring based on text
 * matches in certain index fields.
 * @member {object} [textWeights.weights] The dictionary of per-field weights
 * to boost document scoring. The keys are field names and the values are the
 * weights for each field.
 * @member {array} [functions] The collection of functions that influence the
 * scoring of documents.
 * @member {string} [functionAggregation] A value indicating how the results of
 * individual scoring functions should be combined. Defaults to "Sum". Ignored
 * if there are no scoring functions. Possible values include: 'sum',
 * 'average', 'minimum', 'maximum', 'firstMatching'
 */
export interface ScoringProfile {
  name: string;
  textWeights?: TextWeights;
  functions?: ScoringFunction[];
  functionAggregation?: string;
}

/**
 * @class
 * Initializes a new instance of the CorsOptions class.
 * @constructor
 * Defines options to control Cross-Origin Resource Sharing (CORS) for an
 * index.
 *
 * @member {array} allowedOrigins The list of origins from which JavaScript
 * code will be granted access to your index. Can contain a list of hosts of
 * the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a single
 * '*' to allow all origins (not recommended).
 * @member {number} [maxAgeInSeconds] The duration for which browsers should
 * cache CORS preflight responses. Defaults to 5 mintues.
 */
export interface CorsOptions {
  allowedOrigins: string[];
  maxAgeInSeconds?: number;
}

/**
 * @class
 * Initializes a new instance of the Suggester class.
 * @constructor
 * Defines how the Suggest API should apply to a group of fields in the index.
 *
 * @member {string} name The name of the suggester.
 * @member {array} sourceFields The list of field names to which the suggester
 * applies. Each field must be searchable.
 */
export interface Suggester {
  name: string;
  sourceFields: string[];
}

/**
 * @class
 * Initializes a new instance of the Index class.
 * @constructor
 * Represents an index definition in Azure Search, which describes the fields
 * and search behavior of an index.
 *
 * @member {string} name The name of the index.
 * @member {array} fields The fields of the index.
 * @member {array} [scoringProfiles] The scoring profiles for the index.
 * @member {string} [defaultScoringProfile] The name of the scoring profile to
 * use if none is specified in the query. If this property is not set and no
 * scoring profile is specified in the query, then default scoring (tf-idf)
 * will be used.
 * @member {object} [corsOptions] Options to control Cross-Origin Resource
 * Sharing (CORS) for the index.
 * @member {array} [corsOptions.allowedOrigins] The list of origins from which
 * JavaScript code will be granted access to your index. Can contain a list of
 * hosts of the form {protocol}://{fully-qualified-domain-name}[:{port#}], or a
 * single '*' to allow all origins (not recommended).
 * @member {number} [corsOptions.maxAgeInSeconds] The duration for which
 * browsers should cache CORS preflight responses. Defaults to 5 mintues.
 * @member {array} [suggesters] The suggesters for the index.
 * @member {array} [analyzers] The analyzers for the index.
 * @member {array} [tokenizers] The tokenizers for the index.
 * @member {array} [tokenFilters] The token filters for the index.
 * @member {array} [charFilters] The character filters for the index.
 * @member {string} [eTag] The ETag of the index.
 */
export interface Index {
  name: string;
  fields: Field[];
  scoringProfiles?: ScoringProfile[];
  defaultScoringProfile?: string;
  corsOptions?: CorsOptions;
  suggesters?: Suggester[];
  analyzers?: Analyzer[];
  tokenizers?: Tokenizer[];
  tokenFilters?: TokenFilter[];
  charFilters?: CharFilter[];
  eTag?: string;
}

/**
 * @class
 * Initializes a new instance of the IndexListResult class.
 * @constructor
 * Response from a List Indexes request. If successful, it includes the full
 * definitions of all indexes.
 *
 * @member {array} [indexes] The indexes in the Search service.
 */
export interface IndexListResult {
  readonly indexes?: Index[];
}

/**
 * @class
 * Initializes a new instance of the SearchRequestOptions class.
 * @constructor
 * Additional parameters for a set of operations.
 *
 * @member {uuid} [clientRequestId] The tracking ID sent with the request to
 * help with debugging.
 */
export interface SearchRequestOptions {
  clientRequestId?: string;
}

/**
 * @class
 * Initializes a new instance of the AccessCondition class.
 * @constructor
 * Additional parameters for a set of operations.
 *
 * @member {string} [ifMatch] Defines the If-Match condition. The operation
 * will be performed only if the ETag on the server matches this value.
 * @member {string} [ifNoneMatch] Defines the If-None-Match condition. The
 * operation will be performed only if the ETag on the server does not match
 * this value.
 */
export interface AccessCondition {
  ifMatch?: string;
  ifNoneMatch?: string;
}

