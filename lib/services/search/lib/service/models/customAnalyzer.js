/*
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License. See License.txt in the project root for
 * license information.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is
 * regenerated.
 */

'use strict';

const models = require('./index');

/**
 * Allows you to take control over the process of converting text into
 * indexable/searchable tokens. It's a user-defined configuration consisting of
 * a single predefined tokenizer and one or more filters. The tokenizer is
 * responsible for breaking text into tokens, and the filters for modifying
 * tokens emitted by the tokenizer.
 *
 * @extends models['Analyzer']
 */
class CustomAnalyzer extends models['Analyzer'] {
  /**
   * Create a CustomAnalyzer.
   * @member {object} tokenizer The name of the tokenizer to use to divide
   * continuous text into a sequence of tokens, such as breaking a sentence
   * into words.
   * @member {string} [tokenizer.name]
   * @member {array} [tokenFilters] A list of token filters used to filter out
   * or modify the tokens generated by a tokenizer. For example, you can
   * specify a lowercase filter that converts all characters to lowercase. The
   * filters are run in the order in which they are listed.
   * @member {array} [charFilters] A list of character filters used to prepare
   * input text before it is processed by the tokenizer. For instance, they can
   * replace certain characters or symbols. The filters are run in the order in
   * which they are listed.
   */
  constructor() {
    super();
  }

  /**
   * Defines the metadata of CustomAnalyzer
   *
   * @returns {object} metadata of CustomAnalyzer
   *
   */
  mapper() {
    return {
      required: false,
      serializedName: '#Microsoft.Azure.Search.CustomAnalyzer',
      type: {
        name: 'Composite',
        className: 'CustomAnalyzer',
        modelProperties: {
          name: {
            required: true,
            serializedName: 'name',
            type: {
              name: 'String'
            }
          },
          odatatype: {
            required: true,
            serializedName: '@odata\\.type',
            type: {
              name: 'String'
            }
          },
          tokenizer: {
            required: true,
            serializedName: 'tokenizer',
            type: {
              name: 'Composite',
              className: 'TokenizerName'
            }
          },
          tokenFilters: {
            required: false,
            serializedName: 'tokenFilters',
            type: {
              name: 'Sequence',
              element: {
                  required: false,
                  serializedName: 'TokenFilterNameElementType',
                  type: {
                    name: 'Composite',
                    className: 'TokenFilterName'
                  }
              }
            }
          },
          charFilters: {
            required: false,
            serializedName: 'charFilters',
            type: {
              name: 'Sequence',
              element: {
                  required: false,
                  serializedName: 'CharFilterNameElementType',
                  type: {
                    name: 'Composite',
                    className: 'CharFilterName'
                  }
              }
            }
          }
        }
      }
    };
  }
}

module.exports = CustomAnalyzer;
