/*
 * Copyright (c) Microsoft Corporation. All rights reserved.
 * Licensed under the MIT License. See License.txt in the project root for license information.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */

import { BaseResource, CloudError } from "ms-rest-azure";
import * as moment from "moment";

export {

  BaseResource,
  CloudError
};

/**
 * The Usage Names.
 */
export interface UsageName {
  /**
   * The name of the resource.
   */
  readonly value?: string;
  /**
   * The localized name of the resource.
   */
  readonly localizedValue?: string;
}

/**
 * Describes Batch AI Resource Usage.
 */
export interface Usage {
  /**
   * An enum describing the unit of usage measurement. Possible values include: 'Count'
   */
  readonly unit?: string;
  /**
   * The current usage of the resource.
   */
  readonly currentValue?: number;
  /**
   * The maximum permitted usage of the resource.
   */
  readonly limit?: number;
  /**
   * The name of the type of usage.
   */
  readonly name?: UsageName;
}

/**
 * Settings for user account that gets created on each on the nodes of a cluster.
 */
export interface UserAccountSettings {
  /**
   * @summary User name.
   * @description Name of the administrator user account which can be used to SSH to nodes.
   */
  adminUserName: string;
  /**
   * @summary SSH public key.
   * @description SSH public key of the administrator user account.
   */
  adminUserSshPublicKey?: string;
  /**
   * @summary Password.
   * @description Password of the administrator user account.
   */
  adminUserPassword?: string;
}

/**
 * SSH configuration.
 */
export interface SshConfiguration {
  /**
   * @summary Allowed public IPs.
   * @description List of source IP ranges to allow SSH connection from. The default value is '*'
   * (all source IPs are allowed). Maximum number of IP ranges that can be specified is 400.
   */
  publicIPsToAllow?: string[];
  /**
   * @summary User account settings.
   * @description Settings for administrator user account to be created on a node. The account can
   * be used to establish SSH connection to the node.
   */
  userAccountSettings: UserAccountSettings;
}

/**
 * Data disks settings.
 */
export interface DataDisks {
  /**
   * @summary Disk size in GB.
   * @description Disk size in GB for the blank data disks.
   */
  diskSizeInGB: number;
  /**
   * @summary Caching type.
   * @description Caching type for the disks. Available values are none (default), readonly,
   * readwrite. Caching type can be set only for VM sizes supporting premium storage. Possible
   * values include: 'none', 'readonly', 'readwrite'
   */
  cachingType?: string;
  /**
   * @summary Number of data disks.
   * @description Number of data disks attached to the File Server. If multiple disks attached,
   * they will be configured in RAID level 0.
   */
  diskCount: number;
  /**
   * @summary Storage account type.
   * @description Type of storage account to be used on the disk. Possible values are: Standard_LRS
   * or Premium_LRS. Premium storage account type can only be used with VM sizes supporting premium
   * storage. Possible values include: 'Standard_LRS', 'Premium_LRS'
   */
  storageAccountType: string;
}

/**
 * Represents a resource ID. For example, for a subnet, it is the resource URL for the subnet.
 */
export interface ResourceId extends BaseResource {
  /**
   * The ID of the resource
   */
  id: string;
}

/**
 * File Server mount Information.
 */
export interface MountSettings {
  /**
   * @summary Mount Point.
   * @description Path where the data disks are mounted on the File Server.
   */
  mountPoint?: string;
  /**
   * @summary Public IP.
   * @description Public IP address of the File Server which can be used to SSH to the node from
   * outside of the subnet.
   */
  fileServerPublicIP?: string;
  /**
   * @summary Internal IP.
   * @description Internal IP address of the File Server which can be used to access the File
   * Server from within the subnet.
   */
  fileServerInternalIP?: string;
}

/**
 * A definition of an Azure proxy resource.
 */
export interface ProxyResource extends BaseResource {
  /**
   * The ID of the resource.
   */
  readonly id?: string;
  /**
   * The name of the resource.
   */
  readonly name?: string;
  /**
   * The type of the resource.
   */
  readonly type?: string;
}

/**
 * File Server information.
 */
export interface FileServer extends ProxyResource {
  /**
   * @summary VM size.
   * @description VM size of the File Server.
   */
  vmSize?: string;
  /**
   * @summary SSH configuration.
   * @description SSH configuration for accessing the File Server node.
   */
  sshConfiguration?: SshConfiguration;
  /**
   * @summary Data disks configuration.
   * @description Information about disks attached to File Server VM.
   */
  dataDisks?: DataDisks;
  /**
   * @summary Subnet.
   * @description File Server virtual network subnet resource ID.
   */
  subnet?: ResourceId;
  /**
   * @summary Mount settings.
   * @description File Server mount settings.
   */
  readonly mountSettings?: MountSettings;
  /**
   * @summary Provisioning State Transition time.
   * @description Time when the provisioning state was changed.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * @summary Creation time.
   * @description Time when the FileServer was created.
   */
  readonly creationTime?: Date;
  /**
   * @summary Provisioning state.
   * @description Provisioning state of the File Server. Possible values: creating - The File
   * Server is getting created; updating - The File Server creation has been accepted and it is
   * getting updated; deleting - The user has requested that the File Server be deleted, and it is
   * in the process of being deleted; failed - The File Server creation has failed with the
   * specified error code. Details about the error code are specified in the message field;
   * succeeded - The File Server creation has succeeded. Possible values include: 'creating',
   * 'updating', 'deleting', 'succeeded', 'failed'
   */
  readonly provisioningState?: string;
}

/**
 * Key Vault Secret reference.
 */
export interface KeyVaultSecretReference {
  /**
   * @summary Key Vault resource identifier.
   * @description Fully qualified resource identifier of the Key Vault.
   */
  sourceVault: ResourceId;
  /**
   * @summary Secret URL.
   * @description The URL referencing a secret in the Key Vault.
   */
  secretUrl: string;
}

/**
 * File Server creation parameters.
 */
export interface FileServerCreateParameters {
  /**
   * @summary VM size.
   * @description The size of the virtual machine for the File Server. For information about
   * available VM sizes from the Virtual Machines Marketplace, see Sizes for Virtual Machines
   * (Linux).
   */
  vmSize: string;
  /**
   * @summary SSH configuration.
   * @description SSH configuration for the File Server node.
   */
  sshConfiguration: SshConfiguration;
  /**
   * @summary Data disks.
   * @description Settings for the data disks which will be created for the File Server.
   */
  dataDisks: DataDisks;
  /**
   * @summary Subnet identifier.
   * @description Identifier of an existing virtual network subnet to put the File Server in. If
   * not provided, a new virtual network and subnet will be created.
   */
  subnet?: ResourceId;
}

/**
 * Manual scale settings for the cluster.
 */
export interface ManualScaleSettings {
  /**
   * @summary Target node count.
   * @description The desired number of compute nodes in the Cluster. Default is 0.
   */
  targetNodeCount: number;
  /**
   * @summary Node deallocation options.
   * @description An action to be performed when the cluster size is decreasing. The default value
   * is requeue. Possible values include: 'requeue', 'terminate', 'waitforjobcompletion'
   */
  nodeDeallocationOption?: string;
}

/**
 * Auto-scale settings for the cluster. The system automatically scales the cluster up and down
 * (within minimumNodeCount and maximumNodeCount) based on the number of queued and running jobs
 * assigned to the cluster.
 */
export interface AutoScaleSettings {
  /**
   * @summary Minimum node count.
   * @description The minimum number of compute nodes the Batch AI service will try to allocate for
   * the cluster. Note, the actual number of nodes can be less than the specified value if the
   * subscription has not enough quota to fulfill the request.
   */
  minimumNodeCount: number;
  /**
   * @summary Maximum node count.
   * @description The maximum number of compute nodes the cluster can have.
   */
  maximumNodeCount: number;
  /**
   * @summary Initial node count.
   * @description The number of compute nodes to allocate on cluster creation. Note that this value
   * is used only during cluster creation. Default: 0.
   */
  initialNodeCount?: number;
}

/**
 * At least one of manual or autoScale settings must be specified. Only one of manual or autoScale
 * settings can be specified. If autoScale settings are specified, the system automatically scales
 * the cluster up and down (within the supplied limits) based on the pending jobs on the cluster.
 */
export interface ScaleSettings {
  /**
   * @summary Manual scale settings.
   * @description Manual scale settings for the cluster.
   */
  manual?: ManualScaleSettings;
  /**
   * @summary Auto-scale settings.
   * @description Auto-scale settings for the cluster.
   */
  autoScale?: AutoScaleSettings;
}

/**
 * The OS image reference.
 */
export interface ImageReference {
  /**
   * @summary Publisher.
   * @description Publisher of the image.
   */
  publisher: string;
  /**
   * @summary Offer.
   * @description Offer of the image.
   */
  offer: string;
  /**
   * @summary SKU.
   * @description SKU of the image.
   */
  sku: string;
  /**
   * @summary Version.
   * @description Version of the image.
   */
  version?: string;
  /**
   * @summary Custom VM image resource ID.
   * @description The ARM resource identifier of the virtual machine image for the compute nodes.
   * This is of the form
   * /subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/providers/Microsoft.Compute/images/{imageName}.
   * The virtual machine image must be in the same region and subscription as the cluster. For
   * information about the firewall settings for the Batch node agent to communicate with the Batch
   * service see
   * https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration.
   * Note, you need to provide publisher, offer and sku of the base OS image of which the custom
   * image has been derived from.
   */
  virtualMachineImageId?: string;
}

/**
 * VM configuration.
 */
export interface VirtualMachineConfiguration {
  /**
   * @summary Image reference.
   * @description OS image reference for cluster nodes.
   */
  imageReference?: ImageReference;
}

/**
 * An environment variable definition.
 */
export interface EnvironmentVariable {
  /**
   * @summary Name.
   * @description The name of the environment variable.
   */
  name: string;
  /**
   * @summary Value.
   * @description The value of the environment variable.
   */
  value: string;
}

/**
 * An environment variable with secret value definition.
 */
export interface EnvironmentVariableWithSecretValue {
  /**
   * @summary Name.
   * @description The name of the environment variable to store the secret value.
   */
  name: string;
  /**
   * @summary Value.
   * @description The value of the environment variable. This value will never be reported back by
   * Batch AI.
   */
  value?: string;
  /**
   * @summary KeyVault secret reference.
   * @description KeyVault store and secret which contains the value for the environment variable.
   * One of value or valueSecretReference must be provided.
   */
  valueSecretReference?: KeyVaultSecretReference;
}

/**
 * Specifies a setup task which can be used to customize the compute nodes of the cluster.
 */
export interface SetupTask {
  /**
   * @summary Command line.
   * @description The command line to be executed on each cluster's node after it being allocated
   * or rebooted. The command is executed in a bash subshell as a root.
   */
  commandLine: string;
  /**
   * @summary Environment variables.
   * @description A collection of user defined environment variables to be set for setup task.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * @summary Secrets.
   * @description A collection of user defined environment variables with secret values to be set
   * for the setup task. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * @summary Output path prefix.
   * @description The prefix of a path where the Batch AI service will upload the stdout, stderr
   * and execution log of the setup task.
   */
  stdOutErrPathPrefix: string;
  /**
   * @summary Output path suffix.
   * @description A path segment appended by Batch AI to stdOutErrPathPrefix to form a path where
   * stdout, stderr and execution log of the setup task will be uploaded. Batch AI creates the
   * setup task output directories under an unique path to avoid conflicts between different
   * clusters. The full path can be obtained by concatenation of stdOutErrPathPrefix and
   * stdOutErrPathSuffix.
   */
  readonly stdOutErrPathSuffix?: string;
}

/**
 * Azure storage account credentials.
 */
export interface AzureStorageCredentialsInfo {
  /**
   * @summary Account key.
   * @description Storage account key. One of accountKey or accountKeySecretReference must be
   * specified.
   */
  accountKey?: string;
  /**
   * @summary Account key secret reference.
   * @description Information about KeyVault secret storing the storage account key. One of
   * accountKey or accountKeySecretReference must be specified.
   */
  accountKeySecretReference?: KeyVaultSecretReference;
}

/**
 * Azure File Share mounting configuration.
 */
export interface AzureFileShareReference {
  /**
   * @summary Account name.
   * @description Name of the Azure storage account.
   */
  accountName: string;
  /**
   * @summary Azure File URL.
   * @description URL to access the Azure File.
   */
  azureFileUrl: string;
  /**
   * @summary Credentials.
   * @description Information about the Azure storage credentials.
   */
  credentials: AzureStorageCredentialsInfo;
  /**
   * @summary Relative mount path.
   * @description The relative path on the compute node where the Azure File share will be mounted.
   * Note that all cluster level file shares will be mounted under $AZ_BATCHAI_MOUNT_ROOT location
   * and all job level file shares will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * @summary File mode.
   * @description File mode for files on the mounted file share. Default value: 0777.
   */
  fileMode?: string;
  /**
   * @summary Directory mode.
   * @description File mode for directories on the mounted file share. Default value: 0777.
   */
  directoryMode?: string;
}

/**
 * Azure Blob Storage Container mounting configuration.
 */
export interface AzureBlobFileSystemReference {
  /**
   * @summary Account name.
   * @description Name of the Azure storage account.
   */
  accountName: string;
  /**
   * @summary Container name.
   * @description Name of the Azure Blob Storage container to mount on the cluster.
   */
  containerName: string;
  /**
   * @summary Credentials.
   * @description Information about the Azure storage credentials.
   */
  credentials: AzureStorageCredentialsInfo;
  /**
   * @summary Relative mount path.
   * @description The relative path on the compute node where the Azure File container will be
   * mounted. Note that all cluster level containers will be mounted under $AZ_BATCHAI_MOUNT_ROOT
   * location and all job level containers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * @summary Mount options.
   * @description Mount options for mounting blobfuse file system.
   */
  mountOptions?: string;
}

/**
 * File Server mounting configuration.
 */
export interface FileServerReference {
  /**
   * @summary File server.
   * @description Resource ID of the existing File Server to be mounted.
   */
  fileServer: ResourceId;
  /**
   * @summary Source directory.
   * @description File Server directory that needs to be mounted. If this property is not
   * specified, the entire File Server will be mounted.
   */
  sourceDirectory?: string;
  /**
   * @summary Relative mount path.
   * @description The relative path on the compute node where the File Server will be mounted. Note
   * that all cluster level file servers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and
   * all job level file servers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * @summary Mount options.
   * @description Mount options to be passed to mount command.
   */
  mountOptions?: string;
}

/**
 * Unmanaged file system mounting configuration.
 */
export interface UnmanagedFileSystemReference {
  /**
   * @summary Mount command.
   * @description Mount command line. Note, Batch AI will append mount path to the command on its
   * own.
   */
  mountCommand: string;
  /**
   * @summary Relative mount path.
   * @description The relative path on the compute node where the unmanaged file system will be
   * mounted. Note that all cluster level unmanaged file systems will be mounted under
   * $AZ_BATCHAI_MOUNT_ROOT location and all job level unmanaged file systems will be mounted under
   * $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
}

/**
 * Details of volumes to mount on the cluster.
 */
export interface MountVolumes {
  /**
   * @summary Azure File Shares.
   * @description A collection of Azure File Shares that are to be mounted to the cluster nodes.
   */
  azureFileShares?: AzureFileShareReference[];
  /**
   * @summary Azure Blob file systems.
   * @description A collection of Azure Blob Containers that are to be mounted to the cluster
   * nodes.
   */
  azureBlobFileSystems?: AzureBlobFileSystemReference[];
  /**
   * @summary File Servers.
   * @description A collection of Batch AI File Servers that are to be mounted to the cluster
   * nodes.
   */
  fileServers?: FileServerReference[];
  /**
   * @summary Unmanaged file systems.
   * @description A collection of unmanaged file systems that are to be mounted to the cluster
   * nodes.
   */
  unmanagedFileSystems?: UnmanagedFileSystemReference[];
}

/**
 * Azure Application Insights information for performance counters reporting.
 */
export interface AppInsightsReference {
  /**
   * @summary Component ID.
   * @description Azure Application Insights component resource ID.
   */
  component: ResourceId;
  /**
   * @summary Instrumentation Key.
   * @description Value of the Azure Application Insights instrumentation key.
   */
  instrumentationKey?: string;
  /**
   * @summary Instrumentation key KeyVault Secret reference.
   * @description KeyVault Store and Secret which contains Azure Application Insights
   * instrumentation key. One of instrumentationKey or instrumentationKeySecretReference must be
   * specified.
   */
  instrumentationKeySecretReference?: KeyVaultSecretReference;
}

/**
 * Performance counters reporting settings.
 */
export interface PerformanceCountersSettings {
  /**
   * @summary Azure Application Insights reference.
   * @description Azure Application Insights information for performance counters reporting. If
   * provided, Batch AI will upload node performance counters to the corresponding Azure
   * Application Insights account.
   */
  appInsightsReference: AppInsightsReference;
}

/**
 * Node setup settings.
 */
export interface NodeSetup {
  /**
   * @summary Setup task.
   * @description Setup task to run on cluster nodes when nodes got created or rebooted. The setup
   * task code needs to be idempotent. Generally the setup task is used to download static data
   * that is required for all jobs that run on the cluster VMs and/or to download/install software.
   */
  setupTask?: SetupTask;
  /**
   * @summary Mount volumes.
   * @description Mount volumes to be available to setup task and all jobs executing on the
   * cluster. The volumes will be mounted at location specified by $AZ_BATCHAI_MOUNT_ROOT
   * environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * @summary Performance counters settings.
   * @description Settings for performance counters collecting and uploading.
   */
  performanceCountersSettings?: PerformanceCountersSettings;
}

/**
 * Counts of various compute node states on the cluster.
 */
export interface NodeStateCounts {
  /**
   * @summary Idle node count.
   * @description Number of compute nodes in idle state.
   */
  readonly idleNodeCount?: number;
  /**
   * @summary Running node count.
   * @description Number of compute nodes which are running jobs.
   */
  readonly runningNodeCount?: number;
  /**
   * @summary Preparing node count.
   * @description Number of compute nodes which are being prepared.
   */
  readonly preparingNodeCount?: number;
  /**
   * @summary Unusable node count.
   * @description Number of compute nodes which are in unusable state.
   */
  readonly unusableNodeCount?: number;
  /**
   * @summary Leaving node count.
   * @description Number of compute nodes which are leaving the cluster.
   */
  readonly leavingNodeCount?: number;
}

/**
 * Cluster creation operation.
 */
export interface ClusterCreateParameters {
  /**
   * @summary VM size.
   * @description The size of the virtual machines in the cluster. All nodes in a cluster have the
   * same VM size. For information about available VM sizes for clusters using images from the
   * Virtual Machines Marketplace see Sizes for Virtual Machines (Linux). Batch AI service supports
   * all Azure VM sizes except STANDARD_A0 and those with premium storage (STANDARD_GS,
   * STANDARD_DS, and STANDARD_DSV2 series).
   */
  vmSize: string;
  /**
   * @summary VM priority.
   * @description VM priority. Allowed values are: dedicated (default) and lowpriority. Possible
   * values include: 'dedicated', 'lowpriority'
   */
  vmPriority?: string;
  /**
   * @summary Scale settings.
   * @description Scale settings for the cluster. Batch AI service supports manual and auto scale
   * clusters.
   */
  scaleSettings?: ScaleSettings;
  /**
   * @summary VM configuration.
   * @description OS image configuration for cluster nodes. All nodes in a cluster have the same OS
   * image.
   */
  virtualMachineConfiguration?: VirtualMachineConfiguration;
  /**
   * @summary Node setup.
   * @description Setup to be performed on each compute node in the cluster.
   */
  nodeSetup?: NodeSetup;
  /**
   * @summary User account settings.
   * @description Settings for an administrator user account that will be created on each compute
   * node in the cluster.
   */
  userAccountSettings: UserAccountSettings;
  /**
   * @summary Subnet.
   * @description Existing virtual network subnet to put the cluster nodes in. Note, if a File
   * Server mount configured in node setup, the File Server's subnet will be used automatically.
   */
  subnet?: ResourceId;
}

/**
 * Cluster update parameters.
 */
export interface ClusterUpdateParameters {
  /**
   * @summary Scale settings.
   * @description Desired scale settings for the cluster. Batch AI service supports manual and auto
   * scale clusters.
   */
  scaleSettings?: ScaleSettings;
}

/**
 * Name-value pair.
 */
export interface NameValuePair {
  /**
   * @summary Name.
   * @description The name in the name-value pair.
   */
  name?: string;
  /**
   * @summary Value.
   * @description The value in the name-value pair.
   */
  value?: string;
}

/**
 * An error response from the Batch AI service.
 */
export interface BatchAIError {
  /**
   * An identifier of the error. Codes are invariant and are intended to be consumed
   * programmatically.
   */
  readonly code?: string;
  /**
   * A message describing the error, intended to be suitable for display in a user interface.
   */
  readonly message?: string;
  /**
   * A list of additional details about the error.
   */
  readonly details?: NameValuePair[];
}

/**
 * Information about a Cluster.
 */
export interface Cluster extends ProxyResource {
  /**
   * @summary VM size.
   * @description The size of the virtual machines in the cluster. All nodes in a cluster have the
   * same VM size.
   */
  vmSize?: string;
  /**
   * @summary VM priority.
   * @description VM priority of cluster nodes. Possible values include: 'dedicated', 'lowpriority'
   */
  vmPriority?: string;
  /**
   * @summary Scale settings.
   * @description Scale settings of the cluster.
   */
  scaleSettings?: ScaleSettings;
  /**
   * @summary VM configuration.
   * @description Virtual machine configuration (OS image) of the compute nodes. All nodes in a
   * cluster have the same OS image configuration.
   */
  virtualMachineConfiguration?: VirtualMachineConfiguration;
  /**
   * @summary Node setup.
   * @description Setup (mount file systems, performance counters settings and custom setup task)
   * to be performed on each compute node in the cluster.
   */
  nodeSetup?: NodeSetup;
  /**
   * @summary User account settings.
   * @description Administrator user account settings which can be used to SSH to compute nodes.
   */
  userAccountSettings?: UserAccountSettings;
  /**
   * @summary Subnet.
   * @description Virtual network subnet resource ID the cluster nodes belong to.
   */
  subnet?: ResourceId;
  /**
   * @summary Creation time.
   * @description The time when the cluster was created.
   */
  readonly creationTime?: Date;
  /**
   * @summary Provisioning state.
   * @description Provisioning state of the cluster. Possible value are: creating - Specifies that
   * the cluster is being created. succeeded - Specifies that the cluster has been created
   * successfully. failed - Specifies that the cluster creation has failed. deleting - Specifies
   * that the cluster is being deleted. Possible values include: 'creating', 'succeeded', 'failed',
   * 'deleting'
   */
  readonly provisioningState?: string;
  /**
   * @summary Provisioning State Transition time.
   * @description Time when the provisioning state was changed.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * @summary Allocation state.
   * @description Allocation state of the cluster. Possible values are: steady - Indicates that the
   * cluster is not resizing. There are no changes to the number of compute nodes in the cluster in
   * progress. A cluster enters this state when it is created and when no operations are being
   * performed on the cluster to change the number of compute nodes. resizing - Indicates that the
   * cluster is resizing; that is, compute nodes are being added to or removed from the cluster.
   * Possible values include: 'steady', 'resizing'
   */
  readonly allocationState?: string;
  /**
   * @summary Allocation state transition time.
   * @description The time at which the cluster entered its current allocation state.
   */
  readonly allocationStateTransitionTime?: Date;
  /**
   * @summary Errors.
   * @description Collection of errors encountered by various compute nodes during node setup.
   */
  readonly errors?: BatchAIError[];
  /**
   * @summary Current node count.
   * @description The number of compute nodes currently assigned to the cluster.
   */
  readonly currentNodeCount?: number;
  /**
   * @summary Node state counts.
   * @description Counts of various node states on the cluster.
   */
  readonly nodeStateCounts?: NodeStateCounts;
}

/**
 * Credentials to access a container image in a private repository.
 */
export interface PrivateRegistryCredentials {
  /**
   * @summary User name.
   * @description User name to login to the repository.
   */
  username: string;
  /**
   * @summary Password.
   * @description User password to login to the docker repository. One of password or
   * passwordSecretReference must be specified.
   */
  password?: string;
  /**
   * @summary Password secret reference.
   * @description KeyVault Secret storing the password. Users can store their secrets in Azure
   * KeyVault and pass it to the Batch AI service to integrate with KeyVault. One of password or
   * passwordSecretReference must be specified.
   */
  passwordSecretReference?: KeyVaultSecretReference;
}

/**
 * Information about docker image for the job.
 */
export interface ImageSourceRegistry {
  /**
   * @summary Server URL.
   * @description URL for image repository.
   */
  serverUrl?: string;
  /**
   * @summary Image.
   * @description The name of the image in the image repository.
   */
  image: string;
  /**
   * @summary Credentials.
   * @description Credentials to access the private docker repository.
   */
  credentials?: PrivateRegistryCredentials;
}

/**
 * Docker container settings.
 */
export interface ContainerSettings {
  /**
   * @summary Image source registry.
   * @description Information about docker image and docker registry to download the container
   * from.
   */
  imageSourceRegistry: ImageSourceRegistry;
  /**
   * @summary /dev/shm size.
   * @description Size of /dev/shm. Please refer to docker documentation for supported argument
   * formats.
   */
  shmSize?: string;
}

/**
 * CNTK (aka Microsoft Cognitive Toolkit) job settings.
 */
export interface CNTKsettings {
  /**
   * @summary Language type.
   * @description The language to use for launching CNTK (aka Microsoft Cognitive Toolkit) job.
   * Valid values are 'BrainScript' or 'Python'.
   */
  languageType?: string;
  /**
   * @summary Config file path.
   * @description Specifies the path of the BrainScript config file. This property can be specified
   * only if the languageType is 'BrainScript'.
   */
  configFilePath?: string;
  /**
   * @summary Python script file path.
   * @description Python script to execute. This property can be specified only if the languageType
   * is 'Python'.
   */
  pythonScriptFilePath?: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter. This property can be specified only if the
   * languageType is 'Python'.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the python script or cntk
   * executable.
   */
  commandLineArgs?: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * pyTorch job settings.
 */
export interface PyTorchSettings {
  /**
   * @summary Python script file path.
   * @description The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
  /**
   * @summary Communication backend.
   * @description Type of the communication backend for distributed jobs. Valid values are 'TCP',
   * 'Gloo' or 'MPI'. Not required for non-distributed jobs.
   */
  communicationBackend?: string;
}

/**
 * TensorFlow job settings.
 */
export interface TensorFlowSettings {
  /**
   * @summary Python script file path.
   * @description The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Master command line arguments.
   * @description Command line arguments that need to be passed to the python script for the master
   * task.
   */
  masterCommandLineArgs?: string;
  /**
   * @summary Worker command line arguments.
   * @description Command line arguments that need to be passed to the python script for the worker
   * task. Optional for single process jobs.
   */
  workerCommandLineArgs?: string;
  /**
   * @summary Parameter server command line arguments.
   * @description Command line arguments that need to be passed to the python script for the
   * parameter server. Optional for single process jobs.
   */
  parameterServerCommandLineArgs?: string;
  /**
   * @summary Worker count.
   * @description The number of worker tasks. If specified, the value must be less than or equal to
   * (nodeCount * numberOfGPUs per VM). If not specified, the default value is equal to nodeCount.
   * This property can be specified only for distributed TensorFlow training.
   */
  workerCount?: number;
  /**
   * @summary Parameter server count.
   * @description The number of parameter server tasks. If specified, the value must be less than
   * or equal to nodeCount. If not specified, the default value is equal to 1 for distributed
   * TensorFlow training. This property can be specified only for distributed TensorFlow training.
   */
  parameterServerCount?: number;
}

/**
 * Caffe job settings.
 */
export interface CaffeSettings {
  /**
   * @summary Config file path.
   * @description Path of the config file for the job. This property cannot be specified if
   * pythonScriptFilePath is specified.
   */
  configFilePath?: string;
  /**
   * @summary Python script file path.
   * @description Python script to execute. This property cannot be specified if configFilePath is
   * specified.
   */
  pythonScriptFilePath?: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter. The property can be specified only if the
   * pythonScriptFilePath is specified.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the Caffe job.
   */
  commandLineArgs?: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Caffe2 job settings.
 */
export interface Caffe2Settings {
  /**
   * @summary Python script file path.
   * @description The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
}

/**
 * Chainer job settings.
 */
export interface ChainerSettings {
  /**
   * @summary Python script file path.
   * @description The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Custom tool kit job settings.
 */
export interface CustomToolkitSettings {
  /**
   * @summary Command line.
   * @description The command line to execute on the master node.
   */
  commandLine?: string;
}

/**
 * Custom MPI job settings.
 */
export interface CustomMpiSettings {
  /**
   * @summary Command line.
   * @description The command line to be executed by mpi runtime on each compute node.
   */
  commandLine: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Specifies the settings for Horovod job.
 */
export interface HorovodSettings {
  /**
   * @summary Python script file path.
   * @description The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * @summary Python interpreter path.
   * @description The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * @summary Command line arguments.
   * @description Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * @summary Process count.
   * @description Number of processes to launch for the job execution. The default value for this
   * property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Job preparation settings.
 */
export interface JobPreparation {
  /**
   * @summary Command line.
   * @description The command line to execute. If containerSettings is specified on the job, this
   * commandLine will be executed in the same container as job. Otherwise it will be executed on
   * the node.
   */
  commandLine: string;
}

/**
 * Input directory for the job.
 */
export interface InputDirectory {
  /**
   * @summary ID.
   * @description The ID for the input directory. The job can use AZ_BATCHAI_INPUT_<id> environment
   * variable to find the directory path, where <id> is the value of id attribute.
   */
  id: string;
  /**
   * @summary Path.
   * @description The path to the input directory.
   */
  path: string;
}

/**
 * Output directory for the job.
 */
export interface OutputDirectory {
  /**
   * @summary ID.
   * @description The ID of the output directory. The job can use AZ_BATCHAI_OUTPUT_<id>
   * environment variable to find the directory path, where <id> is the value of id attribute.
   */
  id: string;
  /**
   * @summary Path prefix.
   * @description The prefix path where the output directory will be created. Note, this is an
   * absolute path to prefix. E.g. $AZ_BATCHAI_MOUNT_ROOT/MyNFS/MyLogs. The full path to the output
   * directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and
   * pathSuffix.
   */
  pathPrefix: string;
  /**
   * @summary Path suffix.
   * @description The suffix path where the output directory will be created. E.g. models. You can
   * find the full path to the output directory by combining pathPrefix,
   * jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.
   */
  pathSuffix?: string;
}

/**
 * Constraints associated with the Job.
 */
export interface JobBasePropertiesConstraints {
  /**
   * @summary Max wall clock time.
   * @description Max time the job can run. Default value: 1 week.
   */
  maxWallClockTime?: moment.Duration;
}

/**
 * Job creation parameters.
 */
export interface JobCreateParameters {
  /**
   * @summary Scheduling priority.
   * @description Scheduling priority associated with the job. Possible values: low, normal, high.
   * Possible values include: 'low', 'normal', 'high'
   */
  schedulingPriority?: string;
  /**
   * @summary Cluster.
   * @description Resource ID of the cluster on which this job will run.
   */
  cluster: ResourceId;
  /**
   * @summary Mount volumes.
   * @description Information on mount volumes to be used by the job. These volumes will be mounted
   * before the job execution and will be unmounted after the job completion. The volumes will be
   * mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * @summary Node count.
   * @description Number of compute nodes to run the job on. The job will be gang scheduled on that
   * many compute nodes.
   */
  nodeCount: number;
  /**
   * @summary Container settings.
   * @description Docker container settings for the job. If not provided, the job will run directly
   * on the node.
   */
  containerSettings?: ContainerSettings;
  /**
   * @summary CNTK settings.
   * @description Settings for CNTK (aka Microsoft Cognitive Toolkit) job.
   */
  cntkSettings?: CNTKsettings;
  /**
   * @summary pyTorch settings.
   * @description Settings for pyTorch job.
   */
  pyTorchSettings?: PyTorchSettings;
  /**
   * @summary TensorFlow settings.
   * @description Settings for Tensor Flow job.
   */
  tensorFlowSettings?: TensorFlowSettings;
  /**
   * @summary Caffe settings.
   * @description Settings for Caffe job.
   */
  caffeSettings?: CaffeSettings;
  /**
   * @summary Caffe2 settings.
   * @description Settings for Caffe2 job.
   */
  caffe2Settings?: Caffe2Settings;
  /**
   * @summary Chainer settings.
   * @description Settings for Chainer job.
   */
  chainerSettings?: ChainerSettings;
  /**
   * @summary Custom tool kit job.
   * @description Settings for custom tool kit job.
   */
  customToolkitSettings?: CustomToolkitSettings;
  /**
   * @summary Custom MPI settings.
   * @description Settings for custom MPI job.
   */
  customMpiSettings?: CustomMpiSettings;
  /**
   * @summary Horovod settings.
   * @description Settings for Horovod job.
   */
  horovodSettings?: HorovodSettings;
  /**
   * @summary Job preparation.
   * @description A command line to be executed on each node allocated for the job before tool kit
   * is launched.
   */
  jobPreparation?: JobPreparation;
  /**
   * @summary Standard output path prefix.
   * @description The path where the Batch AI service will store stdout, stderror and execution log
   * of the job.
   */
  stdOutErrPathPrefix: string;
  /**
   * @summary Input directories.
   * @description A list of input directories for the job.
   */
  inputDirectories?: InputDirectory[];
  /**
   * @summary Output directories.
   * @description A list of output directories for the job.
   */
  outputDirectories?: OutputDirectory[];
  /**
   * @summary Environment variables.
   * @description A list of user defined environment variables which will be setup for the job.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * @summary Secrets.
   * @description A list of user defined environment variables with secret values which will be
   * setup for the job. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * Constraints associated with the Job.
   */
  constraints?: JobBasePropertiesConstraints;
}

/**
 * Constraints associated with the Job.
 */
export interface JobPropertiesConstraints {
  /**
   * @summary Max wall clock time.
   * @description Max time the job can run. Default value: 1 week.
   */
  maxWallClockTime?: moment.Duration;
}

/**
 * Information about the execution of a job.
 */
export interface JobPropertiesExecutionInfo {
  /**
   * @summary Start time.
   * @description The time at which the job started running. 'Running' corresponds to the running
   * state. If the job has been restarted or retried, this is the most recent time at which the job
   * started running. This property is present only for job that are in the running or completed
   * state.
   */
  readonly startTime?: Date;
  /**
   * @summary End time.
   * @description The time at which the job completed. This property is only returned if the job is
   * in completed state.
   */
  readonly endTime?: Date;
  /**
   * @summary Exit code.
   * @description The exit code of the job. This property is only returned if the job is in
   * completed state.
   */
  readonly exitCode?: number;
  /**
   * @summary Errors.
   * @description A collection of errors encountered by the service during job execution.
   */
  readonly errors?: BatchAIError[];
}

/**
 * Information about a Job.
 */
export interface Job extends ProxyResource {
  /**
   * @summary Scheduling priority.
   * @description Scheduling priority associated with the job. Possible values include: 'low',
   * 'normal', 'high'
   */
  schedulingPriority?: string;
  /**
   * @summary Cluster.
   * @description Resource ID of the cluster associated with the job.
   */
  cluster?: ResourceId;
  /**
   * @summary Mount volumes.
   * @description Collection of mount volumes available to the job during execution. These volumes
   * are mounted before the job execution and unmounted after the job completion. The volumes are
   * mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * @summary Number of compute nodes to run the job on.
   * @description The job will be gang scheduled on that many compute nodes
   */
  nodeCount?: number;
  /**
   * @summary If provided the job will run in the specified container.
   * @description If the container was downloaded as part of cluster setup then the same container
   * image will be used. If not provided, the job will run on the VM.
   */
  containerSettings?: ContainerSettings;
  /**
   * @summary The toolkit type of this job.
   * @description Possible values are: cntk, tensorflow, caffe, caffe2, chainer, pytorch, custom,
   * custommpi, horovod. Possible values include: 'cntk', 'tensorflow', 'caffe', 'caffe2',
   * 'chainer', 'horovod', 'custommpi', 'custom'
   */
  toolType?: string;
  /**
   * @summary Specifies the settings for CNTK (aka Microsoft Cognitive Toolkit) job.
   */
  cntkSettings?: CNTKsettings;
  /**
   * @summary Specifies the settings for pyTorch job.
   */
  pyTorchSettings?: PyTorchSettings;
  /**
   * @summary Specifies the settings for Tensor Flow job.
   */
  tensorFlowSettings?: TensorFlowSettings;
  /**
   * @summary Specifies the settings for Caffe job.
   */
  caffeSettings?: CaffeSettings;
  /**
   * @summary Specifies the settings for Caffe2 job.
   */
  caffe2Settings?: Caffe2Settings;
  /**
   * @summary Specifies the settings for Chainer job.
   */
  chainerSettings?: ChainerSettings;
  /**
   * @summary Specifies the settings for custom tool kit job.
   */
  customToolkitSettings?: CustomToolkitSettings;
  /**
   * @summary Specifies the settings for custom MPI job.
   */
  customMpiSettings?: CustomMpiSettings;
  /**
   * @summary Specifies the settings for Horovod job.
   */
  horovodSettings?: HorovodSettings;
  /**
   * @summary Specifies the actions to be performed before tool kit is launched.
   * @description The specified actions will run on all the nodes that are part of the job
   */
  jobPreparation?: JobPreparation;
  /**
   * @summary Output directory path segment.
   * @description A segment of job's output directories path created by Batch AI. Batch AI creates
   * job's output directories under an unique path to avoid conflicts between jobs. This value
   * contains a path segment generated by Batch AI to make the path unique and can be used to find
   * the output directory on the node or mounted filesystem.
   */
  readonly jobOutputDirectoryPathSegment?: string;
  /**
   * @summary Standard output directory path prefix.
   * @description The path where the Batch AI service stores stdout, stderror and execution log of
   * the job.
   */
  stdOutErrPathPrefix?: string;
  /**
   * @summary Input directories.
   * @description A list of input directories for the job.
   */
  inputDirectories?: InputDirectory[];
  /**
   * @summary Output directories.
   * @description A list of output directories for the job.
   */
  outputDirectories?: OutputDirectory[];
  /**
   * @summary Environment variables.
   * @description A collection of user defined environment variables to be setup for the job.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * @summary Secrets.
   * @description A collection of user defined environment variables with secret values to be setup
   * for the job. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * Constraints associated with the Job.
   */
  constraints?: JobPropertiesConstraints;
  /**
   * @summary Creation time.
   * @description The creation time of the job.
   */
  readonly creationTime?: Date;
  /**
   * @summary Provisioning state.
   * @description The provisioned state of the Batch AI job. Possible values include: 'creating',
   * 'succeeded', 'failed', 'deleting'
   */
  readonly provisioningState?: string;
  /**
   * @summary Provisioning state transition time.
   * @description The time at which the job entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * @summary Execution state.
   * @description The current state of the job. Possible values are: queued - The job is queued and
   * able to run. A job enters this state when it is created, or when it is awaiting a retry after
   * a failed run. running - The job is running on a compute cluster. This includes job-level
   * preparation such as downloading resource files or set up container specified on the job - it
   * does not necessarily mean that the job command line has started executing. terminating - The
   * job is terminated by the user, the terminate operation is in progress. succeeded - The job has
   * completed running successfully and exited with exit code 0. failed - The job has finished
   * unsuccessfully (failed with a non-zero exit code) and has exhausted its retry limit. A job is
   * also marked as failed if an error occurred launching the job. Possible values include:
   * 'queued', 'running', 'terminating', 'succeeded', 'failed'
   */
  readonly executionState?: string;
  /**
   * @summary Execution state transition time.
   * @description The time at which the job entered its current execution state.
   */
  readonly executionStateTransitionTime?: Date;
  /**
   * Information about the execution of a job.
   */
  executionInfo?: JobPropertiesExecutionInfo;
}

/**
 * Login details to SSH to a compute node in cluster.
 */
export interface RemoteLoginInformation {
  /**
   * @summary Node ID.
   * @description ID of the compute node.
   */
  readonly nodeId?: string;
  /**
   * @summary IP address.
   * @description Public IP address of the compute node.
   */
  readonly ipAddress?: string;
  /**
   * @summary Port.
   * @description SSH port number of the node.
   */
  readonly port?: number;
}

/**
 * Properties of the file or directory.
 */
export interface File {
  /**
   * @summary Name.
   * @description Name of the file.
   */
  readonly name?: string;
  /**
   * @summary File type.
   * @description Type of the file. Possible values are file and directory. Possible values
   * include: 'file', 'directory'
   */
  readonly fileType?: string;
  /**
   * @summary Download URL.
   * @description URL to download the corresponding file. The downloadUrl is not returned for
   * directories.
   */
  readonly downloadUrl?: string;
  /**
   * @summary Last modified time.
   * @description The time at which the file was last modified.
   */
  readonly lastModified?: Date;
  /**
   * @summary Content length.
   * @description The file of the size.
   */
  readonly contentLength?: number;
}

/**
 * A definition of an Azure resource.
 */
export interface Resource extends BaseResource {
  /**
   * The ID of the resource
   */
  readonly id?: string;
  /**
   * The name of the resource
   */
  readonly name?: string;
  /**
   * The type of the resource
   */
  readonly type?: string;
  /**
   * The location of the resource
   */
  readonly location?: string;
  /**
   * The tags of the resource
   */
  readonly tags?: { [propertyName: string]: string };
}

/**
 * The object that describes the operation.
 */
export interface OperationDisplay {
  /**
   * @summary Friendly name of the resource provider.
   */
  readonly provider?: string;
  /**
   * @summary The operation type.
   * @description For example: read, write, delete, or listKeys/action
   */
  readonly operation?: string;
  /**
   * @summary The resource type on which the operation is performed.
   */
  readonly resource?: string;
  /**
   * @summary The friendly name of the operation.
   */
  readonly description?: string;
}

/**
 * @summary A REST API operation.
 * @description Details of a REST API operation
 */
export interface Operation {
  /**
   * @summary The operation name.
   * @description This is of the format {provider}/{resource}/{operation}
   */
  readonly name?: string;
  /**
   * The object that describes the operation.
   */
  display?: OperationDisplay;
  /**
   * @summary The intended executor of the operation.
   */
  readonly origin?: string;
  /**
   * @summary Properties of the operation.
   */
  properties?: any;
}

/**
 * Batch AI Workspace information.
 */
export interface Workspace extends Resource {
  /**
   * @summary Creation time.
   * @description Time when the Workspace was created.
   */
  readonly creationTime?: Date;
  /**
   * @summary Provisioning state.
   * @description The provisioned state of the Workspace. Possible values include: 'creating',
   * 'succeeded', 'failed', 'deleting'
   */
  readonly provisioningState?: string;
  /**
   * @summary Provisioning state transition time.
   * @description The time at which the workspace entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
}

/**
 * Workspace creation parameters.
 */
export interface WorkspaceCreateParameters {
  /**
   * @summary Location.
   * @description The region in which to create the Workspace.
   */
  location: string;
  /**
   * @summary Tags.
   * @description The user specified tags associated with the Workspace.
   */
  tags?: { [propertyName: string]: string };
}

/**
 * Workspace update parameters.
 */
export interface WorkspaceUpdateParameters {
  /**
   * @summary Tags.
   * @description The user specified tags associated with the Workspace.
   */
  tags?: { [propertyName: string]: string };
}

/**
 * Experiment information.
 */
export interface Experiment extends ProxyResource {
  /**
   * @summary Creation time.
   * @description Time when the Experiment was created.
   */
  readonly creationTime?: Date;
  /**
   * @summary Provisioning state.
   * @description The provisioned state of the experiment. Possible values include: 'creating',
   * 'succeeded', 'failed', 'deleting'
   */
  readonly provisioningState?: string;
  /**
   * @summary Provisioning state transition time.
   * @description The time at which the experiment entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
}

/**
 * Additional parameters for list operation.
 */
export interface WorkspacesListOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listByResourceGroup operation.
 */
export interface WorkspacesListByResourceGroupOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listByWorkspace operation.
 */
export interface ExperimentsListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listByExperiment operation.
 */
export interface JobsListByExperimentOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listOutputFiles operation.
 */
export interface JobsListOutputFilesOptions {
  /**
   * Id of the job output directory. This is the OutputDirectory-->id parameter that is given by
   * the user during Create Job.
   */
  outputdirectoryid: string;
  /**
   * The path to the directory.
   */
  directory?: string;
  /**
   * The number of minutes after which the download link will expire.
   */
  linkexpiryinminutes?: number;
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listByWorkspace operation.
 */
export interface FileServersListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * Additional parameters for listByWorkspace operation.
 */
export interface ClustersListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be
   * returned.
   */
  maxResults?: number;
}

/**
 * @summary Result of the request to list REST API operations. It contains a list of operations and
 * a URL nextLink to get the next set of results.
 * @description Contains the list of all operations supported by BatchAI resource provider
 */
export interface OperationListResult extends Array<Operation> {
  /**
   * @summary The URL to get the next set of operation list results if there are any.
   */
  readonly nextLink?: string;
}

/**
 * The List Usages operation response.
 */
export interface ListUsagesResult extends Array<Usage> {
  /**
   * The URI to fetch the next page of compute resource usage information. Call ListNext() with
   * this to fetch the next page of compute resource usage information.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List operation.
 */
export interface WorkspaceListResult extends Array<Workspace> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List operation.
 */
export interface ExperimentListResult extends Array<Experiment> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List operation.
 */
export interface JobListResult extends Array<Job> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List operation.
 */
export interface FileListResult extends Array<File> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List operation.
 */
export interface RemoteLoginInformationListResult extends Array<RemoteLoginInformation> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the File Server List operation.
 */
export interface FileServerListResult extends Array<FileServer> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Values returned by the List Clusters operation.
 */
export interface ClusterListResult extends Array<Cluster> {
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}
